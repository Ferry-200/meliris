# Meliris: An Intelligent System for LRC Enhancement based on Vocal Detection

## 模型方案设计

### 一、总体思路

本项目旨在构建一个**结合人声检测与演唱者区分**的 LRC 歌词自动优化系统，
目标是在较小的训练数据量（500 首音乐）下
实现高精度、可解释、具备实时性的预测结果。

在前期实验中，LSTM 模型已经能较好地捕捉人声时序规律，
但在遇到复杂伴奏、间奏或歌声变化时仍存在以下问题：

* 模型对音色和节奏变化敏感度不足；
* 预测稳定性偏低；
* 对人声与间奏的置信度差距有限；
* 模型在噪声和多乐器干扰下容易误判。

### 二、CNN + LSTM 结构

#### 2.1 设计思路

* **CNN（卷积神经网络）** 用于从音频的 Mel 频谱中提取局部时频模式，能捕捉到：

  * 不同频段的谐波结构；
  * 乐器与人声的频谱差异；
  * 人声的共振特征与能量集中区。

* **LSTM（长短期记忆网络）** 用于处理 CNN 输出的时间序列特征，
  建模音乐在时间维度上的依赖关系，如：

  * 人声音节之间的持续与间断；
  * 歌曲结构变化；
  * 间奏的节奏规律。

CNN 提供高层局部特征，LSTM 建立时间关系，
两者结合能够让模型兼具“听觉细节”与“时序记忆”。

#### 2.2 模型结构示意（文本描述）

```
输入：Mel 频谱 (T × F)
 ↓
2D 卷积层（Conv2D + ReLU + Pooling）
 ↓
特征展平 (Flatten)
 ↓
LSTM 层
 ↓
时间分布全连接层（TimeDistributed Dense）
 ↓
Sigmoid 输出（预测每帧为人声的概率）
```

#### 2.3 优点分析

| 特点 | 优势 |
| ---- | ---- |
| CNN 负责局部频谱提取 | 减少频率噪声影响，捕捉音色模式 |
| LSTM 负责时间建模 | 保留上下文语义，处理延续性人声 |
| 结构轻量 | 可在 CPU 或移动设备上运行 |
| 对训练量要求低 | 适合 500 首以内数据集   |

## 最终研究路线：人声活动检测中的标签质量与任务优化分析

### 阶段一：基线模型与弱监督性能评估 (LRC Baseline)

**目标：** 建立基于传统、粗糙文本标注的性能基线（Performance Floor），为后续改进提供严格的对比依据。

| 元素 | 描述 |
| :--- | :--- |
| **标签来源** | **LRC 文本时间戳**（通常是反转后进行加权）。 |
| **监督性质** | 弱监督（Weak Supervision）。 |
| **模型架构** | **单任务：** CNN + LSTM + Sigmoid 输出。 |
| **训练挑战** | LRC 标注不准确，导致模型难以区分弱人声和乐器，边界模糊。 |
| **核心产出** | 模型的基线 F1-Score、Precision 和 Recall。 |

---

### 阶段二：标签优化与强监督单任务性能验证 (Optimized Single-Task)

**目标：** 通过引入 Demucs 人声轨，生成最高质量的二分类标签，验证标签质量提升对模型性能的巨大贡献，并建立最优单任务模型。

| 元素 | 描述 |
| :--- | :--- |
| **标签生成** | **创新点：** 使用 HT Demucs f.t. (v4) 分离的人声轨 $V$ 作为输入，利用**人工&阈值**生成**高质量的二分类标签 $Y_{SVD}$**。 |
| **监督性质** | 强监督（Strong Supervision）。 |
| **模型架构** | **单任务：** 保持 **CNN + LSTM** 架构不变（只替换标签）。 |
| **训练优势** | 新标签 $Y_{SVD}$ 解决了小提琴混淆和弱人声边界不清晰的问题。 |
| **核心产出** | **性能对比：** 严格对比 F1-Score 相较于阶段一的提升幅度（预期是数量级提升）。**证明标签质量优于模型结构复杂度。** |

---

### 阶段三：多任务学习探索与边际效益分析 (MTL Enhancement)

**目标：** 在拥有高质量分类标签 $Y_{SVD}$ 的基础上，探索引入人声比例（Ratio）作为辅助任务是否能带来**额外的边际性能提升**，并分析 MTL 的必要性与成本。

| 元素 | 描述 |
| :--- | :--- |
| **任务设计** | **多任务：** 分类（人声/间奏） + 回归（人声占比 Ratio）。 |
| **模型架构** | **双头输出：** CNN + LSTM 共享编码器，分叉出分类头和回归头。 |
| **标签来源** | 1. **分类标签：** $Y_{manual}$（来自 Demucs&manual 模型）。2. **回归标签：** $\text{Ratio} = V/(V+I)$（来自 Demucs）。 |
| **损失函数** | **联合损失：** $L_{total} = \lambda_{1} L_{cls} + \lambda_{2} L_{reg}$。其中 $L_{reg}$ 应使用 **Loss Masking**（仅在有效人声区域计算回归误差）。 |
| **核心产出** | **必要性论证：** 比较 MTL 模型相较于阶段二单任务模型的 F1-Score 提升是否显著。若提升微小，则结论为“最优单任务模型在工程上更具优势”。 |

### 科学评估与最终总结

您的最终论文将围绕以下三个核心对比点展开：

1.  **标签质量对比：** 证明 $Y_{SVD}$ **显著优于** LRC 标签。
2.  **性能贡献对比：** 证明 **阶段二** 模型（强监督单任务）的性能 **远超** **阶段一** 模型（弱监督基线）。
3.  **任务优化对比：** 论证 **阶段三** 模型（MTL）相较于 **阶段二** 模型（最优单任务）的**边际效益**，得出关于模型复杂度与性能平衡的最终结论。

## 后处理

目前的后处理方案是迟滞阈值

某些情况下，预测会与实际（LRC）有些出入，需要结合 LRC 进行再一次后处理。

- 理想情况：
  ```
  [01:35.00]line1
  [01:40.00]
  [01:50.00]line2
  ```
  LRC 有空行作为上一行歌词的结束标识，
  该空行和下一句歌词之间（如上例的 1:40 -> 1:50 之间），进间奏动画。
  这时直接忽略模型预测，把 LRC 提供的这个结果作为最优先结果。

- 大部分情况：
  ```
  [01:35.00]line1（假设 1:40 line1 就结束了，剩下 1:40 -> 1:50 是间奏）
  [01:50.00]line2
  ```
  大部分 LRC 的歌词时长会延伸到下一句歌词，这可能会包括间奏。
  模型预测的间奏部分有很大可能处于这段时间（1:36 -> 1:50）。

  由于无法保证间奏预测百分百准确，为了不影响下一行歌词展示，
  预测间奏必须在 line2 开始时结束。短则延长，长则截断。
  同理，无法保证预测间奏开始时间一定是 line1 结束的时间，
  可能同时展示 line1 和预测间奏动画是更好的选择。

- 坏的情况：
  间奏预测开始时间和任意一行非空的 LRC 行开始时间相近。
  比如在上面大部分情况中展示的 LRC 中，
  在 1:34 或者 1:36 开始的间奏预测都是不可取的。
  这种情况会影响歌词展示，应该过滤掉这些预测。
  需要估计这个“相近”的时间范围。

  模型给出的间奏预测有较短的中断，在允许范围内允许填补中断。
  需要估计这个“允许中断时长”。

### 方案总结（基于 LRC 行开始时间绝对准确的前提）

- 原则：以每一行 LRC 的开始时间为硬边界，仅在相邻两行开始时间之间的安全区展示间奏。
  - 安全窗口定义：对相邻两行开始时间 `t_i`、`t_{i+1}`，`W_i = [t_i + m_start,\; t_{i+1}]`
  - 任何间奏片段不得跨越下一行开始时间 `t_{i+1}`，且靠近任一行开始的近邻窗内不触发。

- 输入：
  - 迟滞后得到的候选片段集合（由模型的间奏概率 `p(t)` 二值化产生）
  - 每一帧的间奏概率 `p(t)`，人声比率 `voice_ratio(t)`
  - LRC 行开始时间序列 `{t_i}`，以及帧间隔 `Δt`

- 流程：
  1) 迟滞二值化：设进入/退出阈值 `τ_on > τ_off`，对 `p(t)` 进行迟滞，得到初始片段；
  2) 窗口约束：与各 `W_i` 求交，分裂跨窗片段，并将起止贴齐到 `t_i + m_start` 与 `t_{i+1}`；
     细则：
     - 完全落入窗口：`W_i` 内保留，必要时贴齐到边界或片段内更稳点
     - 起点在窗外、终点在窗内：裁剪起点到 `t_i + m_start`
     - 起点在窗内、终点跨出窗外：裁剪终点到 `t_{i+1}`
     - 跨越多个窗口：按每个 `W_k` 求交分裂为子片段，分别评分
     - 空窗或极短窗：`t_{i+1} - t_i ≤ m_start` 时不展示
     - 闭合越界修正：形态学闭合产生的跨窗合并必须在此阶段重新切断到 `t_{i+1}`
     - 覆盖整行歌词的错误预测：`s < t_i` 且 `e > t_{i+1}` 时仅保留与各 `W_k` 的交集部分，其余丢弃
  3) 闭合与清洗：丢弃短于 `min_len` 的片段；填补最大中断时长不超过 `t_gap_fill` 的短中断；
  4) 评分分级：对每个片段计算
     - 平均间奏概率 `P_mean = mean(p(t)|t∈片段)`；
     - 器乐支持度 `V_inst = 1 - mean(voice_ratio(t)|t∈片段)`；
     - 距最近 LRC 开始的最小距离归一化 `g(d_min)`；
     - 进入/退出斜率一致性 `h(s_in,s_out)`；
     - 综合评分 `R = 0.5·P_mean + 0.3·V_inst + 0.1·g + 0.1·h`；
     根据 `R` 进行强/弱分级展示或丢弃；
  5) 近边界处理：若片段起点落入任一 LRC 开始的 ±`t_near` 近邻窗，根据评分 `R` 延后到安全区或直接舍弃；
  6) 自校准：统计冲突率与短中断分布，必要时将 `m_start` 上调 0.1–0.2s，或将 `t_gap_fill` 调至掉点分布的 90% 分位但不超过上限。

- 参数建议（默认值与自适应）：
  - 保护边界：`m_start = 0.6s`；可按拍长 `T = 60/BPM` 自适应，`m_start = clamp(1.0·T, 0.5, 0.8)` 同理；
  - 近邻禁入窗：`t_near = 0.6s`；
  - 最短片段：`min_len = 0.5s` 或 `T`；
  - 中断填补：`t_gap_fill = 0.3s`；
  - 帧换算：`frames = round(seconds / Δt)`，所有秒级参数最终以帧数实现。

- 评估指标：
  - 冲突率：片段起点落入任一 LRC 开始 ±`t_near` 的比例 < 2%；
  - 闪烁次数：迟滞切换数/分钟降低；
  - 有效展示时长：安全窗口内强/弱展示总时长不低于基线；
  - 人声一致性：强展示片段的 `mean(voice_ratio)` 显著低于弱/弃片段。
